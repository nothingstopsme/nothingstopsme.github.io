

<html data-theme="dark">
  <head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	
  <!-- font awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">

  <link rel="stylesheet" href="/Blogs/static/main/css/classless.css">

  <style>

    @import url('https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&display=swap');


    html[data-theme='dark'] {
      /* foreground   | background color */
      --cfg:   #cdcdcd; --cbg:    #1b1b1b;
      --cdark: #999;    --clight: #343434; /*#333;*/
      --cmed:  #566;
      --clink: #1ad;
      --cemph: #0b9;    --cemphbg: #0b91;


      --font-p: 1.2em/1.7 "Fira Mono", monospace;
      --font-h: .9em/1.5 "Fira Mono", monospace; 
      --font-c: .9em/1.4 "Fira Mono", monospace;

      --width: 54rem;
      --navpos: fixed;
	    --ornament: "";
    }

    article * {
      scroll-margin-top: 9rem; 
    }

    p,
    article table{
      margin-top: 3rem;
    }

    article figure {
      text-align: center;
    }

    article figure img {
      background: #cdcdcd;
    }

    article h3:before {
      left: -0.5rem;
      margin-left: 0rem;
      font-size: 1.5rem;
    }

    article h2:before {
      left: -0.5rem;
      margin-left: -2rem;
      font-size: 1.5rem;
    }

    article h2.reference-title:before {
      display: none;
    }

    article ul {
      overflow: hidden;
    }

    

    a[href] {
      text-decoration: none;
    }

    

    nav ul:not(:first-child) li {
      padding: 0;
      margin: 0;
    }


    nav input[type="checkbox"]:hover + label, 
    nav input[type="checkbox"]:focus + label, 
    nav a.nav-active,
    nav a:hover, 
    nav a:focus {
      background: black;
    }


    nav li > a {
      display: block;
    }

    nav > span {
      height: inherit;
      background: inherit;
    }

    nav > span > input[type="checkbox"] {
      width: 0;
    }

    nav > span > input[type="checkbox"] + label { 
      color: var(--clink); cursor: pointer; 
    }


    nav > span > ul {
      background: inherit;
      display: inline-block;
      width: auto;
      margin: 0;
      padding: 0;
    }

    nav > span > input[type="checkbox"] + label {
      height: inherit;
      padding: 1rem 0.6rem;
    }

    nav > span > ul > li {
      height: 4rem;
      display: inline-block;
    }

    nav > span > ul > li > a {
      height: inherit;
      padding: 1rem 0.6rem;
    }

   
    nav > span.left-menu {
      float: left;
    }

    nav > span.left-menu > input[type="checkbox"] + label {
      float: left;
      display: none;
    }

    nav > span.left-menu > input[type="checkbox"] + label + ul {
      float: left;
      clear: left;
      
    }
    
    nav > span.left-menu > input[type="checkbox"] + label + ul > li{
      float: left;
    }

     
    nav > span.right-menu {
      float: right;
    }


    nav > span.right-menu > input[type="checkbox"] + label {
      float: right;
      display: inline-block;
    }

    nav > span.right-menu > input[type="checkbox"] + label + ul {
      display: none;
      float: right;
      clear: right;
      overflow-y: auto;
      max-height: 50vh;

      border: var(--border);
      border-radius: 4px;

    }

    nav > span.right-menu > input[type="checkbox"] + label + ul > li {
      display: block;
    }

    nav > span.right-menu > input[type="checkbox"]:hover + label + ul,
    nav > span.right-menu > input[type="checkbox"]:focus + label + ul,
    nav > span.right-menu > input[type="checkbox"] ~ ul:hover {
        display: inline-block;
    }

   
    body>nav {
      
      left: max(calc(50vw - var(--width)/2), 0vw);
      right: max(calc(50vw - var(--width)/2), 0vw);
      width: auto;
      height: 4rem;
      box-shadow: none;
      z-index: 100;
      overflow-y: visible;

    }

		article div.post-tags span {
			white-space: nowrap;
		}

    article div.post-tags span:not(:first-child):before
    {
			content: "|";
    }
  
   
    @media (max-width: 40rem) 
    {
      nav > span.left-menu > input[type="checkbox"] + label + ul > li {
        float: none;
        display: block;
      }
   
      nav > span.left-menu > input[type="checkbox"] + label {
        display: inline-block;
      }
      
      nav > span.left-menu > input[type="checkbox"] + label + ul {
        display: none;
      }

      nav > span.left-menu > input[type="checkbox"]:hover + label + ul,
      nav > span.left-menu > input[type="checkbox"]:focus + label + ul,
      nav > span.left-menu > input[type="checkbox"] + label + ul:hover {
        display: inline-block;
     

        overflow-y: auto;
        max-height: 50vh;

        border: var(--border);
        border-radius: 4px;
      }
          

    }

  </style>

  <title>
From VQVAE to VBVAE
</title>
  
	
  </head>

<body style="margin-top: 0; padding-top: 0;">
<nav>
  <span class="left-menu">
  <input type="checkbox" id="nav-menu" />
  <label for="nav-menu">
  &nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;
  </label>

  <!--<a href="#">&nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;</a>-->
  <ul>
    <!--<li class="menu-hamburger">
    </li>-->
    
      
          
          <li>
            <a href="/Blogs/archives/"
            
            

            >Archives</a>                            
          </li>
      
          
          <li>
            <a href="/Blogs/tags/"
            
            

            >Tags</a>                            
          </li>
      
    
  </ul>
  </span>

  
  

</nav>

<main style='margin-top: 4rem; padding: 0 2rem; overflow-y: hidden;'>
  <article>
      
<script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    /*inlineMath: [['$','$'], ['\\(','\\)']],*/
    inlineMath: [['$','$']],
    processEscapes: true},
    jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
    extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
    TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
    equationNumbers: {
    autoNumber: "AMS"
    }
  }
});
</script>


<h1>From <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> to <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr></h1>


<div class="post-tags">
  
  <span>
  <i class="fas fa-calendar"></i>&nbsp;<time>18.Jan.2021</time>
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;Probability
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;MachineLearning
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;Research
  </span>
  
  
</div>



<p>Living in the Digital Age, we have witnessed how important that the notion of "digitalisation" is in the advance of modern technologies. Arguably the main advantange brought by digitalisation, or quantisation from the perspective of digital signal processing, is the transformation of complicated signals into a form which can be not only processed and stored efficiently, but also robust to noise. Lately, such an idea has also be deployed into the field of machinge learning, and led to a new model of neural networks called <strong>V</strong>ector <strong>Q</strong>uantised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder. While the modelling method has been proven successful in many applications, issues have been observed in my attempt to incorporate it into my recent project, where <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> have had difficulty to deal with a large code space. Therefore, a new variant named <strong>V</strong>ector <strong>B</strong>inarised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder, which retains the spirit and benefit of quantisation but does not suffer from the shortcomings of the original <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr>, is proposed, and in this post the derivation from <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> to <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> as well as the intuitions behind it are presented. Note that no thorough and comprehensive experiments have been conducted to support this new concept, so please read with a huge grain of salt. ; )</p>
<h2>Review: <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr></h2>
<p>For a data point $x$, the objective (or called <strong>E</strong>vidence <strong>L</strong>ower <strong>BO</strong>und in some literature) of a <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder to be maximised is normally written as:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Obj_{\text{VAE}}(x) = E_{z \sim q(z|x)}[log(p(x|z))] \\ 
& \phantom{Obj_{\text{VAE}}(x) =\;} + E_{z \sim q(z|x)}[log(\frac{p(z)}{q(z|x)})]
\end{aligned}
\label{eq:ELBO_VAE}
\end{equation}
$$</p>
<p>where $z$ is a point in the latent space; $q(z|x)$, $p(x|z)$, and $p(z)$ are the approximate posterior distribution, likelihood, and prior distribution over the joint space of $x$ and $z$, respectively. In the common setup of <abbr title="Variational AutoEncoder">VAE</abbr>, $q(z|x)$ is usually described by some parametric distribution, whose parameters are generated by a neural network (referred to as encoder $En$) taking $x$ as input. Such an arrangement is also adopted in <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr>, excpet that the authors have designed a special distribution for $q(z|x)$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& q(z = e_k|x) \\
& = \begin{cases}
1 \;\;\; \text{if }e_k = argmin_{e_i \in \{e_1,\dotsc,e_K\}} \| En(x) - e_i \|_2 \\
0 \;\;\; \text{otherwise}
\end{cases}
\end{aligned}
\label{eq:q_VQVAE}
\end{equation}
$$</p>
<p>with $\{e_1,\dotsc,e_K\}$ being a codebook, a set of $K$ codes of which the dimensionnality is the same as the one of $En(x)$. This particular choice turns the process of sampling from $q(z|x)$ into the one which simply maps $En(x)$ to one of the codes via nearest-neighbour-search, and hence the latent space is effectively constrained to contain only $K$ instead of an infinite number of points; in other words, under \eqref{eq:q_VQVAE} the whole latent space can be considered quantised into $K$ discrete values, and $z$ is just the quantised result of $En(x)$ (formulated as $z = Qn(En(x))$). Note that by assuming the prior is a uniform distribution, i.e. $p(z) = \frac{1}{K}, z \in \{e_1,\dotsc,e_K\}$, \eqref{eq:ELBO_VAE} in this case can be further simplified to:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Obj_{\text{Qn}}(x) = log(p(x|Qn(En(x))) + log(\underbrace{p(Qn(En(x)))}_{= \frac{1}{K}}) \\
& \phantom{Obj_{Qn}(x) =} + \underbrace{Entropy(q(z|x))}_{= 0} \\
& \phantom{Obj_{Qn}(x)} = log(p(x|Qn(En(x))) + log(\frac{1}{K})
\end{aligned}
\label{eq:ELBO_Qn}
\end{equation}
$$</p>
<p>which implies for the purpose of maximisation, the effective objective is simply $log(p(x|Qn(En(x)))$. Practically, the direct optimisation of $log(p(x|Qn(En(x)))$ via gradient ascent is impossible, since there is no gradient defined for the quantisation function $Qn$; as a result, the authors of <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> proposed an alternative where an straight-through gradient estimator is used to pass directly to $En(x)$ the gradients that are supposed into $Qn$, and this bypassing operation can be realised by the stop-gradient operator (denoted as $SG$):</p>
<p>$$
\begin{equation}
\begin{aligned}
& Obj_{\text{VQVAE}}(x) \\ 
& = log(p(x| SG(Qn(En(x)) - En(x)) + En(x))) \\ 
& \phantom{=\;}+ \beta \| En(x) - SG(Qn(En(x))) \|_2^2 \\
\end{aligned}
\label{eq:ELBO_VQVAE}
\end{equation}
$$</p>
<p>Note that the second term in \eqref{eq:ELBO_VQVAE} is purposely introduced to serve as a regulariser weighted by the hyper-parameter $\beta$. It corresponds to the fact that other objectives, which will differ from the goal described by \eqref{eq:ELBO_Qn}, are required to provide training signals for $Qn$, as they no longer receive any gradients under \eqref{eq:ELBO_VQVAE}. Once additional optimisation targets dedicated to $Qn$ are incorporated, there will be two disjoint gradient flows (one for parameters involved in the first term of \eqref{eq:ELBO_VQVAE}, and the other for the codebook involved in $Qn$) present in the training dynamic; therefore it needs some form of regularisation, such as the one introduced here, to keep their updates synchronised and make sure the output space of $En$ is learned at the same pace as the codeboock is.</p>
<p>In relation to providing training signals for updating the codebook, one popular strategy is to regard the training of the codebook used in nearest-neighbour-search as K-mean learning, in which codes (or mean vectors from the perspective of K-mean) are updated via <em>on-line EM</em> with the following rules:</p>
<p>$$
\begin{equation}
\begin{cases}
\begin{aligned}
& N_i^{(t)} =  N_i^{(t-1)} * \gamma + n_i^{(t)} * (1-\gamma)
\end{aligned}\\
\begin{aligned}
& e_i^{(t)} = e_i^{(t-1)} * \gamma + \sum_{\substack{j \\ Qn(En(x_j)) = e_i^{(t-1)}}} \frac{En(x_j)}{N_i^{(t)}} * (1 - \gamma)
\end{aligned}
\end{cases}
\label{eq:K_Mean_online_EM}
\end{equation}
$$</p>
<p>where $N_i^{(t)}$ is the average use count for code $i$ up to iteration $t$; $n_i^{(t)}$ is the recorded usage count for code $i$ at iteration $t$; and $\gamma$ is the decaying rate. Despite being effective in many scenarios, training the codebook in this way when the number of codes becomes large could suffer from <em>index collapse</em>, where the utilisation of codebook capacity is inefficient due to the increasing difficulty of exploring and updating the entire code space under nearest-neighbour-search.</p>
<h2>Alternative Interpretation of Quantisation in <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> <a name="alternative_interpretatino"></a></h2>
<p>In the previous section, the quantisation function $Qn$ was introduced to neatly describe the action of sampling from the distribution \eqref{eq:q_VQVAE}, but there was no explicit definition given. Now it is time to make up for it by expressing the nearest-neighbour-search with the condition from \eqref{eq:q_VQVAE}:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Qn(v) \\
& = argmin_{e_i \in \{e_1,\dotsc,e_K\}} \| v - e_i \|_2 \\
& = argmax_{e_i \in \{e_1,\dotsc,e_K\}} -\frac{1}{2} \| v - e_i \|_2^2 \\
& = argmax_{e_i \in \{e_1,\dotsc,e_K\}} -\frac{1}{2} \|v \|_2^2 + v \odot e_i - \frac{1}{2} \| e_i \|_2^2 \\
& = argmax_{e_i \in \{e_1,\dotsc,e_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v \odot e_i - \frac{1}{2} \| e_i \|_2^2)
\end{aligned}
\label{eq:Qn_rewriting}
\end{equation}
$$</p>
<p>Next, let $C$ denote a matrix where the i-th column corresponds to the code $e_i$, $b$ denote a column vector where the i-th entry is equal to $-\frac{1}{2} \| e_i \|_2^2$, and $h_i$ denote a one-hot column vector where the i-th entry is 1; the last equation of \eqref{eq:Qn_rewriting} can be rewritten in the following way:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Qn(v) \\
& = C (argmax_{h_i \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)) \\
& = C (argmax_{h_i \in \{h_1,\dotsc,h_K\}} \frac{exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)}{Z}) \\
& = C h_{max} = e_{max}
\end{aligned}
\label{eq:Qn_special_RBM}
\end{equation}
$$</p>
<p>where $h_{max}$ and $e_{max}$ are used to refer to the corresponding vectors picked under the $argmax$ operator; and $Z$ is a constant with respect to all $h_i$. If $Z$ is deliberately chosen to be $\sum_{h_j \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_j + b^T h_j)$, then $h_{max}$ can be viewed as the result of picking the most likely outcome from the categorical distribution $P(h_i|v)$, which is defined as:</p>
<p>$$
\begin{equation}
\begin{aligned}
& P(h_i|v) \\
& = \frac{exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)}{\sum\limits_{h_j \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_j + b^T h_j)} \\
& = \frac{exp(v^T C h_i + b^T h_i)}{\sum\limits_{h_j \in \{h_1,\dotsc,h_K\}} exp(v^T C h_j + b^T h_j)} \\
\end{aligned}
\label{eq:P_h_i}
\end{equation}
$$</p>
<p>and that is exactly the expression one would derive for the conditional distribution (hidden given visible) from a special case of <strong>R</strong>estricted <strong>B</strong>oltzmann <strong>M</strong>achines, in which binary hidden variables are grouped together to represent a categorical one; visible variables are continuous with its covariance equal to $I$; and the corresponding energy function is $E(v, h_i) = \frac{1}{2} \|v \|_2^2 - v^T C h_i - b^T h_i$. Moreover, as such a <abbr title="Restricted Boltzmann Machines">RBM</abbr> also suggests $P(v|h_i) = N(C h_i, I)$, \eqref{eq:Qn_special_RBM} essentially depicts $Qn(v)$ as taking one Gibbs step starting from $v$, except that there is no randomness involved: samples at the first half of the step ($v \to h_{max}$) are always the outcomes with highest probability, while samples at the second half of the step ($h_{max} \to e_{max}$) are simply the distribution mean.</p>
<h2>Generalising to <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr></h2>
<p>The <abbr title="Restricted Boltzmann Machines">RBM</abbr> interpretation not only sheds light on how the quantisation function in a <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> works, but also suggests that very component needed to compose a quantisation function: A <abbr title="Restricted Boltzmann Machines">RBM</abbr> with discrete hidden variables and continuous visible variables. So it is possible to define other forms of qunantisation by introducing different configurations of RBMs which provide the similar functionality, for example, the generic one with binary hidden and guassian visible variables (denoted by $h$ and $v$ respectively as in <a href="#alternative_interpretatino">the previous section</a>), whose energy function and conditionals are:</p>
<p>$$
\begin{equation}
\begin{cases}
\begin{aligned}
& E_{\text{bi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h - a^T h \\
\end{aligned} \\
\begin{aligned}
& P_{\text{bi}}(h|v) = \frac{exp(-E_{\text{bi}}(v, h))}{\sum_{\hat{h}}exp(-E_{\text{bi}}(v, \hat{h}))} \\
& \phantom{P_{\text{bi}}(h|v)} = \frac{exp(v^T \Sigma^{-0.5} C h + a^T h)}{\sum_{\hat{h}} exp(v^T \Sigma^{-0.5} C \hat{h} + a^T \hat{h})} \\
& \phantom{P_{\text{bi}}(h|v)} = \frac{exp((\overbrace{v^T \Sigma^{-0.5} C + a^T}^{= S(v)^T}) h)}{\sum_{\hat{h}} exp((\underbrace{v^T \Sigma^{-0.5} C + a^T}_{= S(v)^T}) \hat{h})} \\
& \phantom{P_{\text{bi}}(h|v)} = \prod_{m=1}^{K} \frac{exp(S(v)[m] * h[m])}{\sum_{\hat{h}[m] \in \{0, 1\}} exp(S(v)[m] * \hat{h}[m])} \\
& \phantom{P_{\text{bi}}(h|v)} \propto \prod_{m=1}^{K} \sigma(S(v)[m])
\end{aligned}\\
\begin{aligned}
& P_{\text{bi}}(v|h) = \frac{exp(-E_{\text{bi}}(v, h))}{\int_{\hat{v}}exp(-E_{\text{bi}}(\hat{v}, h))} \\ 
& \phantom{P_{\text{bi}}(v|h)} \propto N(\mu + \Sigma^{0.5} C h, \Sigma)
\end{aligned}
\end{cases}
\label{eq:RGBM}
\end{equation}
$$</p>
<p>where $\mu$ is the mean vector; $\Sigma$ is the covariance matrix with all off-diagnal entries equal to 0; $C$ is the same matrix as defined previously; $a$ is the bias vector (which is a free parameter not related to the length of codes, unlike $b$ in \eqref{eq:Qn_special_RBM}); and $S(v) = C \Sigma^{-0.5} v + a$. Also note that under this <abbr title="Restricted Boltzmann Machines">RBM</abbr>, $h$ is no longer a one-hot vector but a bit string of which each entry contains either a bit 1 or a bit 0. With \eqref{eq:RGBM}, a new quantisation function can be defined accordingly:</p>
<p>$$
\begin{align}
& Qn_{\text{bi}}(v) \nonumber \\
& = \mu + \Sigma^{0.5} C h_{max} \nonumber \\
& = \mu + \Sigma^{0.5} C (argmax_{h} exp(-E_{\text{bi}}(v, h))) \nonumber \\
& = \mu + \Sigma^{0.5} C (argmax_{h} \underbrace{\frac{exp(-E_{\text{bi}}(v, h))}{\sum_{\hat{h}} exp(-E_{\text{bi}}(v, h))}}_{= P_{\text{bi}}(h|v)}) \nonumber \\
& = \mu + \Sigma^{0.5} C (argmax_{h} \prod_m P_{\text{bi}}(h[m]|v)) \nonumber \\
& = \mu + \Sigma^{0.5} C 
\begin{bmatrix} 
argmax_{h[1]} P_{\text{bi}}(h[1]|v) \\
\vdots \\
argmax_{h[K]} P_{\text{bi}}(h[K]|v)
\end{bmatrix} \nonumber \\
& = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\lfloor \sigma(S(v)[1]) + 0.5 \rfloor \\
\vdots \\
\lfloor \sigma(S(v)[K]) + 0.5 \rfloor
\end{bmatrix} \nonumber \\
& = \mu + \Sigma^{0.5} C \lfloor \sigma(S(v)) + 0.5 \rfloor \label{eq:Qn_bi} \\
& = e_{max} \nonumber
\end{align}
$$</p>
<p>with $\sigma(.)$ denoting the entrywise sigmoid function and $\lfloor . \rfloor$ representing the entrywise floor operator. Compared with \eqref{eq:Qn_special_RBM}, the troubling $argmax$ operators, which are the cause of undefined gradients for $Qn$, has disappeared in \eqref{eq:Qn_bi}. While the introduction of floor operators induces another blockage of gradient flows, the same straight-through gradient estimator can still be utilised to send training signals over; in fact, such an estimation becomes more effective and accurate when applied to \eqref{eq:Qn_bi} (see <a href="#Qn_vs_Qn_bi.png">Figure 1</a>), as not only does the number of nodes along the gradient path affected by the rerouting is reduced, but no parameters are excluded due to the bypass, which means all parameters can receive the training signals generated with respect to the true objective.</p>
<p id="Qn_vs_Qn_bi.png">
<figure><img alt="Qn_vs_Qn_bi.png" src="/Blogs/static/main/images/VBVAE/Qn_vs_Qn_bi.png" /><figcaption>The comparison between the straight-through gradient estimator for $Qn(v)$ (left) and the one for $Qn_{\text{bi}}(v)$ (right)</figcaption>
</figure></p>
<p>Finally, by the substitution of $Qn_{\text{bi}}$ for $Qn$ in \eqref{eq:ELBO_Qn}, an new variant of <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr>, named <strong>V</strong>ector <strong>B</strong>inarised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder, is established. The notion of binarisation, as the obvious distinction between them, comes from the fact that under $Qn_{\text{bi}}$, discrete codes are indexed by $h$ of the form of bit strings instead of one-hot vectors; as a result, picking a representing code in this quantisation process can be viewed as though deciding whether each bit in $h$ should be on or off. Besides, the objective for <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> highlights another difference:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Obj_{\text{VBVAE}}(x) \\ 
& = log\Bigg(p\bigg(x| SG\Big( \lfloor \sigma(S(En(x))) + 0.5 \rfloor - \sigma(S(En(x))) \Big) \\ 
& \phantom{= log\Bigg(p\bigg(x|\;} + \sigma(S(En(x))) \bigg)\Bigg) \\
\end{aligned}
\label{eq:ELBO_VBVAE}
\end{equation}
$$</p>
<p>which unlike <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> does not require a separate optimisation target to provide training signals for the codebook, and therefore involves no regularisers.</p>
<h2>The Advantanges of <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> over <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr></h2>
<p>Firstly, as mentioned earlier, all parameters in <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> receive (estimate) gradients with respect to \eqref{eq:ELBO_Qn} directly. That allows the removal of additional objectives and regularisers which are essential in <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr> to the contrary, so the distortion of the true optimisation contour that those extra terms might bring about could be avoided.</p>
<p>Secondly, since codes in <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> are indexed by bit strings, the number of distinct codes that a codebook is capable of representing can be as large as $2^{\|\text{the size of the codebook}\|}$; in other words, to model a discrete space containing $K$ codes might only take a codebook of the size $log_2(K)$, rather than $K$ as required in <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr>.</p>
<p>Thirdly, instead of \eqref{eq:K_Mean_online_EM} in <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr>, where individual codes are trained only when being used, in <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> the quantisation to any code always involves all parameters in the codebook; that implies every update in <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> will revise the whole code space, or equivalently, revise every code in it, regardless of which code being used. Hence <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> is considered less susceptible to the issue of <em>index collapse</em>. </p>
<p>Overall, the efficiency in code use and training suggests the competence of <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> for handling large code spaces.</p>
<h2 class="reference-title">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1711.00937">A&#x00E4;ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. “Neural Discrete RepresentationLearning”. In:CoRR abs/1711.00937 (2017)</a></li>
<li><a href="https://arxiv.org/abs/1803.03382">Lukasz Kaiser et al. “Fast Decoding in Sequence Models using Discrete Latent Variables”.In:CoRRabs/1803.03382 (2018)</a></li>
<li><a href="https://arxiv.org/abs/1906.00446">Ali Razavi, A&#x00E4;ron van den Oord, and Oriol Vinyals. “Generating Diverse High-Fidelity Images with VQ-<abbr title="Variational AutoEncoder">VAE</abbr>-2”. In:CoRRabs/1906.00446 (2019)</a></li>
</ul>

</br></br></br></br>

  </article>
</main>

</body>
</html>
