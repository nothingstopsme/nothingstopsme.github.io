

<html data-theme="dark">
  <head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	
  <!-- font awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">

  <link rel="stylesheet" href="/Blogs/static/main/css/classless.css">

  <style>

    @import url('https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&display=swap');


    html[data-theme='dark'] {
      /* foreground   | background color */
      --cfg:   #cdcdcd; --cbg:    #1b1b1b;
      --cdark: #999;    --clight: #343434; /*#333;*/
      --cmed:  #566;
      --clink: #1ad;
      --cemph: #0b9;    --cemphbg: #0b91;


      --font-p: 1.2em/1.7 "Fira Mono", monospace;
      --font-h: .9em/1.5 "Fira Mono", monospace; 
      --font-c: .9em/1.4 "Fira Mono", monospace;

      --width: 54rem;
      --navpos: fixed;
	    --ornament: "";
    }

    article * {
      scroll-margin-top: 9rem; 
    }

    p,
    article table{
      margin-top: 3rem;
    }

    article figure {
      text-align: center;
    }

    article figure img {
      background: #cdcdcd;
    }

    article h3:before {
      left: -0.5rem;
      margin-left: 0rem;
      font-size: 1.5rem;
    }

    article h2:before {
      left: -0.5rem;
      margin-left: -2rem;
      font-size: 1.5rem;
    }

    article h2.reference-title:before {
      display: none;
    }

    article ul {
      overflow: hidden;
    }

    

    a[href] {
      text-decoration: none;
    }

    

    nav ul:not(:first-child) li {
      padding: 0;
      margin: 0;
    }


    nav input[type="checkbox"]:hover + label, 
    nav input[type="checkbox"]:focus + label, 
    nav a.nav-active,
    nav a:hover, 
    nav a:focus {
      background: black;
    }


    nav li > a {
      display: block;
    }

    nav > span {
      height: inherit;
      background: inherit;
    }

    nav > span > input[type="checkbox"] {
      width: 0;
    }

    nav > span > input[type="checkbox"] + label { 
      color: var(--clink); cursor: pointer; 
    }


    nav > span > ul {
      background: inherit;
      display: inline-block;
      width: auto;
      margin: 0;
      padding: 0;
    }

    nav > span > input[type="checkbox"] + label {
      height: inherit;
      padding: 1rem 0.6rem;
    }

    nav > span > ul > li {
      height: 4rem;
      display: inline-block;
    }

    nav > span > ul > li > a {
      height: inherit;
      padding: 1rem 0.6rem;
    }

   
    nav > span.left-menu {
      float: left;
    }

    nav > span.left-menu > input[type="checkbox"] + label {
      float: left;
      display: none;
    }

    nav > span.left-menu > input[type="checkbox"] + label + ul {
      float: left;
      clear: left;
      
    }
    
    nav > span.left-menu > input[type="checkbox"] + label + ul > li{
      float: left;
    }

     
    nav > span.right-menu {
      float: right;
    }


    nav > span.right-menu > input[type="checkbox"] + label {
      float: right;
      display: inline-block;
    }

    nav > span.right-menu > input[type="checkbox"] + label + ul {
      display: none;
      float: right;
      clear: right;
      overflow-y: auto;
      max-height: 50vh;

      border: var(--border);
      border-radius: 4px;

    }

    nav > span.right-menu > input[type="checkbox"] + label + ul > li {
      display: block;
    }

    nav > span.right-menu > input[type="checkbox"]:hover + label + ul,
    nav > span.right-menu > input[type="checkbox"]:focus + label + ul,
    nav > span.right-menu > input[type="checkbox"] ~ ul:hover {
        display: inline-block;
    }

   
    body>nav {
      
      left: max(calc(50vw - var(--width)/2), 0vw);
      right: max(calc(50vw - var(--width)/2), 0vw);
      width: auto;
      height: 4rem;
      box-shadow: none;
      z-index: 100;
      overflow-y: visible;

    }

		article div.post-tags span {
			white-space: nowrap;
		}

    article div.post-tags span:not(:first-child):before
    {
			content: "|";
    }
  
   
    @media (max-width: 40rem) 
    {
      nav > span.left-menu > input[type="checkbox"] + label + ul > li {
        float: none;
        display: block;
      }
   
      nav > span.left-menu > input[type="checkbox"] + label {
        display: inline-block;
      }
      
      nav > span.left-menu > input[type="checkbox"] + label + ul {
        display: none;
      }

      nav > span.left-menu > input[type="checkbox"]:hover + label + ul,
      nav > span.left-menu > input[type="checkbox"]:focus + label + ul,
      nav > span.left-menu > input[type="checkbox"] + label + ul:hover {
        display: inline-block;
     

        overflow-y: auto;
        max-height: 50vh;

        border: var(--border);
        border-radius: 4px;
      }
          

    }

  </style>

  <title>
Improving VBVAE
</title>
  
	
  </head>

<body style="margin-top: 0; padding-top: 0;">
<nav>
  <span class="left-menu">
  <input type="checkbox" id="nav-menu" />
  <label for="nav-menu">
  &nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;
  </label>

  <!--<a href="#">&nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;</a>-->
  <ul>
    <!--<li class="menu-hamburger">
    </li>-->
    
      
          
          <li>
            <a href="/Blogs/archives/"
            
            

            >Archives</a>                            
          </li>
      
          
          <li>
            <a href="/Blogs/tags/"
            
            

            >Tags</a>                            
          </li>
      
    
  </ul>
  </span>

  
  

</nav>

<main style='margin-top: 4rem; padding: 0 2rem; overflow-y: hidden;'>
  <article>
      
<script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    /*inlineMath: [['$','$'], ['\\(','\\)']],*/
    inlineMath: [['$','$']],
    processEscapes: true},
    jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
    extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
    TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
    equationNumbers: {
    autoNumber: "AMS"
    }
  }
});
</script>


<h1>Improving <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr></h1>


<div class="post-tags">
  
  <span>
  <i class="fas fa-calendar"></i>&nbsp;<time>03.Oct.2023</time>
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;Probability
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;MachineLearning
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;Research
  </span>
  
  
</div>



<p>Designed for modelling a huge code space efficiently, <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> (if you are unfamiliar with that acronym, please refer to this <a href="../VBVAE">post</a>) has the potential and looks promising in theory; but in practice it has been observed that its capability can sometimes be undermined by the slowness of code exploration during training and the resulting inferior code space utilisation. So in this post I would like to share three simple tweaks that I have found helpful in facilitating the training of <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr>.</p>
<p>For the sake of convenience, let us first borrow the symbol definition from <a href="../VBVAE">here</a> and write down the quantisation function $Qn_{\text{bi}}(v)$ of <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> given the <abbr title="Restricted Boltzmann Machines">RBM</abbr> interpretation with the energy function $E_{\text{bi}}(v, h)$ and the conditional $P_{\text{bi}}(h|v)$:</p>
<p>$$
\begin{align}
& E_{\text{bi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h - a^T h
 \label{eq_E_bi_v_h} \\
& P_{\text{bi}}(h|v) = \frac{exp(-E_{\text{bi}}(v, h))}{\sum_{\hat{h}}exp(-E_{\text{bi}}(v, \hat{h}))} \nonumber \\
& \phantom{P_{\text{bi}}(h|v)} = \frac{exp((\overbrace{v^T \Sigma^{-0.5} C + a^T}^{= S(v)^T}) h)}{\sum_{\hat{h}} exp((\underbrace{v^T \Sigma^{-0.5} C + a^T}_{= S(v)^T}) \hat{h})} \nonumber \\
& \phantom{P_{\text{bi}}(h|v)} = \prod_{m=1}^{K} \frac{exp(S(v)[m] * h[m])}{\sum_{\hat{h}[m] \in \{0, 1\}} exp(S(v)[m] * \hat{h}[m])} \label{eq_P_bi_h_v} \\
& \phantom{P_{\text{bi}}(h|v)} = \prod_{m=1}^{K} P_{\text{bi}}(h[m]|v) \nonumber \\
& \phantom{P_{\text{bi}}(h|v)} \propto \prod_{m=1}^{K} \sigma(S(v)[m]) \nonumber \\
& Qn_{\text{bi}}(v) = \mu + \Sigma^{0.5} C (argmax_{h}P_{\text{bi}}(h|v)) \nonumber \\
& \phantom{Qn_{\text{bi}}(v)} = \mu + \Sigma^{0.5} C (argmax_{h} \prod_{m=1}^{K} P_{\text{bi}}(h[m]|v)) \nonumber \\
& \phantom{Qn_{\text{bi}}(v)} = \mu + \Sigma^{0.5} C
\begin{bmatrix}
argmax_{h[1]} P_{\text{bi}}(h[1]|v) \\
\vdots \\
argmax_{h[K]} P_{\text{bi}}(h[K]|v)
\end{bmatrix} \label{eq_Qn_bi_v} \\
& \phantom{Qn_{\text{bi}}(v)} = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\lfloor \sigma(S(v)[1]) + 0.5 \rfloor \\
\vdots \\
\lfloor \sigma(S(v)[K]) + 0.5 \rfloor
\end{bmatrix} \nonumber
\end{align}
$$</p>
<h2 id="tweak_1">Tweak 1: using $-1$/$+1$ binarisation as opposed to $0$/$1$</h2>
<p>While the gradients for updating the codebook in <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> are already denser than <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr>, those learning signals can be made even stronger with $-1$/$+1$ binarisation, to always have gradients flow to every component of the codebook embedding sapce, instead of shutting them down when the corresponding bit is $0$.</p>
<p>Because of the <abbr title="Restricted Boltzmann Machines">RBM</abbr> interpretation, it is rather straightforward to derive the quantisation equation $Qn_{\text{-+}}(v)$ for $-1$/$+1$ binarisation, by applying the new discrete set to each entry of $h$ in $\eqref{eq_P_bi_h_v}$:</p>
<p>$$
\begin{aligned}
& P_{\text{-+}}(h|v) = \prod_{m=1}^{K} \frac{exp(S(v)[m] * h[m])}{\sum_{\hat{h}[m] \in \{-1, 1\}} exp(S(v)[m] * \hat{h}[m])} \\
& \phantom{P_{\text{-+}}(h|v)} = \prod_{m=1}^{K} \frac{exp(S(v)[m] * h[m])}{exp(S(v)[m]) + exp(-S(v)[m])} \\
& \phantom{P_{\text{-+}}(h|v)} = \prod_{m=1}^{K} P_{\text{-+}}(h[m]|v) \\
& P_{\text{-+}}(h[m]|v) =
\begin{cases}
1.0 - (tanh(S(v)[m]) + 1.0) * 0.5, \; if \; h[m] = -1 \\
(tanh(S(v)[m]) + 1.0) * 0.5, \; if \; h[m] = 1 \\
\end{cases}
\end{aligned}
$$</p>
<p>then rewriting $\eqref{eq_Qn_bi_v}$ accordingly:</p>
<p>$$
\begin{align}
& Qn_{\text{-+}}(v) = \mu + \Sigma^{0.5} C
\begin{bmatrix}
argmax_{h[1]} P_{\text{-+}}(h[1]|v) \\
\vdots \\
argmax_{h[K]} P_{\text{-+}}(h[K]|v)
\end{bmatrix} \nonumber \\
& \phantom{Qn_{\text{-+}}(v)} = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\begin{cases}
-1, \; (tanh(S(v)[1]) + 1.0) * 0.5 < 0.5 \\
1, \; (tanh(S(v)[1]) + 1.0) * 0.5 \ge 0.5 \\
\end{cases} \\
\vdots \\
\end{bmatrix} \nonumber \\
& \phantom{Qn_{\text{-+}}(v)} = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\begin{cases}
-1, \; tanh(S(v)[1]) < 0 \\
1, \; tanh(S(v)[1]) \ge 0 \\
\end{cases} \\
\vdots \\
\end{bmatrix} \nonumber \\
& \phantom{Qn_{\text{-+}}(v)} = \mu + \Sigma^{0.5} C
\begin{bmatrix}
sign(tanh(S(v)[1])) \\
\vdots \\
sign(tanh(S(v)[K]))  \\
\end{bmatrix} \label{eq_Qn_minus_plus_v}
\end{align}
$$</p>
<p>where</p>
<p>$$
\begin{aligned}
& sign(x) =
\begin{cases}
-1, \; \text{if} \; x < 0 \\
1, \; \text{otherwise} \\
\end{cases}
\end{aligned}
$$</p>
<h2 id="tweak_2">Tweak 2: injecting autoregressive dependency into binary strings</h2>
<p>It is well known that autoregression has the capacity of modelling complicated sequences of data; so the idea is with this technique <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> can produce various binary strings which might be difficult to yield otherwise, resulting in better code space utilisation given the same training time.</p>
<p>One way to inject such dependency is to modify $\eqref{eq_E_bi_v_h}$ and include an extra auxiliary variable $h_a$, of which the content is assumed given and exactly equal to $h$, plus a function $Ar()$ expressing the autoregressive relationship:</p>
<p>$$
\begin{aligned}
& E_{\text{a}}(v, h, h_a) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h - (a+Ar(h_a))^T h \\
& \phantom{E_{\text{a}}(v, h, h_a)} = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h \\
& \phantom{E_{\text{a}}(v, h, h_a) =} - a^T h - \sum_i \underbrace{\big( \sum_{1 \le k < i} A[i,k] h_a[k] \big) }_{Ar(h_a)[i]} h[i] \\
& \phantom{E_{\text{a}}(v, h, h_a)} = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - (v^T \Sigma^{-0.5} C + a^T) h \\
& \phantom{E_{\text{a}}(v, h, h_a) =} - \sum_i \big( \sum_{1 \le k < i} A[i,k] h_a[k] \big) h[i]
\end{aligned}
$$</p>
<p>with $A[i,k]$ being the weight of $h_a[k]$ in a linear combination formulated for $h[i]$. Following this setup, the new conditional of $h$ will be conditioned on both $v$ and $h_a$</p>
<p>$$
\begin{align}
& P_{\text{a}}(h|v, h_a) = \frac{exp((\overbrace{v^T \Sigma^{-0.5} C + a^T}^{S(v)^T} + Ar(h_a)^T) h)}{\sum_{\hat{h}} exp((\underbrace{v^T \Sigma^{-0.5} C + a^T}_{S(v)^T} + Ar(h_a)^T) \hat{h})} \label{eq_P_a_h_v_h_a} \\
& \phantom{P_{\text{a}}(h|v, h_a)} = \prod_{m=1}^K \frac{exp((S(v)[m] + Ar(h_a)[m]) h[m])}{\sum_{\hat{h}[m]} exp((S(v)[m] + Ar(h_a)[m]) \hat{h}[m])} \nonumber \\
& \phantom{P_{\text{a}}(h|v, h_a)} = \prod_{m=1}^K \frac{exp((S(v)[m] + \sum_{1 \le k < m} A[m,k] h_a[k]) h[m])}{\sum_{\hat{h}[m]} exp((S(v)[m] + \sum_{1 \le k < m} A[m,k] h_a[k]) \hat{h}[m])} \nonumber \\
& \phantom{P_{\text{a}}(h|v, h_a)} = \prod_{m=1}^K P_{\text{a}}(h[m]|v, h_{l,< m}) \nonumber
\end{align}
$$</p>
<p>In the case of $-1$/$+1$ binarisation as discussed in <a href="#tweak_1">tweak 1</a>, the conditional above becomes:</p>
<p>$$
\begin{aligned}
& P_{\text{a,-+}}(h|v, h_a) = \prod_{m=1}^K \frac{exp((S(v)[m] + \sum_{1 \le k < m} A[m,k] h_a[k]) h[m])}{\sum_{\hat{h}[m] \in \{-1, +1\}} exp((S(v)[m] + \sum_{1 \le k < m} A[m,k] h_a[k]) \hat{h}[m])} \\
& \phantom{P_{\text{a,-+}}(h|v, h_a)} = \prod_{m=1}^K P_{\text{a,-+}}(h[m]|v, h_a) \\
& P_{\text{a,-+}}(h[m]|v, h_a) \\
& =
\begin{cases}
1.0 - (tanh(S(v)[m] + \sum_{1 \le k < m} A[m,k] h_a[k]) + 1.0) * 0.5, \; if \; h[m] = -1 \\
(tanh(S(v)[m] + \sum_{1 \le k < m} A[m,k] h_a[k]) + 1.0) * 0.5, \; if \; h[m] = 1 \\
\end{cases}
\end{aligned}
$$</p>
<p>which suggests we only need to update the $tanh()$ terms in $\eqref{eq_Qn_minus_plus_v}$ to obtain the corresponding quantisation function $Qn_{\text{a,-+}}(v)$:</p>
<p>$$
\begin{align}
& Qn_{\text{a,-+}}(v) = \mu + \Sigma^{0.5} C
\underbrace{\begin{bmatrix}
sign(tanh(S(v)[1])) \\
\vdots \\
sign(tanh(S(v)[m]+\sum_{1 \le k < m} A[m,k] h_a[k]))  \\
\vdots \\
sign(tanh(S(v)[K]+\sum_{1 \le k < K} A[K,k] h_a[k]))  \\
\end{bmatrix}}_{h = h_a} \label{eq_Qn_a_minus_plus}
\end{align}
$$</p>
<p>Note that because $h_a$ is assumed given, entries of $h$ in $Qn_{\text{a,-+}}(v)$ needs to be calculated sequentially (autoregressively) from $1$ to $K$, in order to fulfill that assumption. While this could imply longer computation time due to lack of parallelism, the number of sequential steps $K$, and the consequential time consumption, would not usually go beyond manageability, as the size of <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr>'s code space grows exponetially with respect to $K$ and a samll $K$ might well be sufficient for offering a necessary capacity.</p>
<p>Additionally, since this tweak makes our posterior estimate, denoted as $q(z|x)$ with $z$ and $x$ being hidden and visible random variable, emit an autoregressive relationship across bits, a prior $p(z)$ which describes data in a similar way might be a better option in this scenario, for facilitating matching between $q(z|x)$ and $p(z)$ and reducing the negative impacts of regularisation. Therefore, instead of adopting the approach of <a href="../VBVAE"><abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr></a> assuming each bit is i.i.d and its two outcomes are eqaully likely so as to drop the <abbr title="Kullbackâ€“Leibler divergence">KL</abbr> term in the objective, an autoregressive prior supporting bit-string modelling could be used, for example, <abbr title="Mixtures of Logistic distributions for Bit-strings">MoLB</abbr> as suggested in my previous <a href="../AutoregressiveMoLB/#sampling_bit-strings">post</a>, with the reintroduction of the omitted terms involving the prior:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Obj_{Qn_{\text{a,-+}}}(x) = log\left(p(x|z=Qn_{\text{a,-+}}(En(x)))\right) \\
& \phantom{Obj_{Qn_{\text{a,-+}}}(x) =} + log\left(\frac{p(z=Qn_{\text{a,-+}}(En(x)))}{q(z = Qn_{\text{a,-+}}(En(x)))|x)}\right) \\
& \phantom{Obj_{Qn_{\text{a,-+}}}(x)} = log\left(p(x|z=Qn_{\text{a,-+}}(En(x)))\right) \\
& \phantom{Obj_{Qn_{\text{a,-+}}}(x) =} + log\left(p(z=Qn_{\text{a,-+}}(En(x)))\right) \\
& \phantom{Obj_{Qn_{\text{a,-+}}}(x) =} - \underbrace{log\left(q(z = Qn_{\text{a,-+}}(En(x))|x)\right)}_{=\; log(1) \; = \; 0} \\
& \phantom{Obj_{Qn_{\text{a,-+}}}(x)} = log\left(p(x|z=Qn_{\text{a,-+}}(En(x)))\right) \\
& \phantom{Obj_{Qn_{\text{a,-+}}}(x) =} + log\left(p(z=Qn_{\text{a,-+}}(En(x)))\right) \\
\end{aligned}
\end{equation}
$$</p>
<p>where $p(x|z)$ is the likelihood and $En(x)$ is the encoder (possibly parameterised via a neural network) that maps $x$ to the input space of the quantisation function $Qn_{\text{a,-+}}(v)$. This capability of working with an parameterised prior is another testimony to the flexibility of <abbr title="Vector Binarised Variational AutoEncoder">VBVAE</abbr> over <abbr title="Vector Quantised Variational AutoEncoder">VQVAE</abbr>, in that moving gradient estimator to the bit level allows (approximate) gradients to flow more freely, even back to the parameters of the prior!</p>
<h2>Tweak 3: decoupling shared parameters</h2>
<p>More specifically, $S(v)$ derived from the conditional of $h$ and the quantisation function itself share two matrices $C$ and $\Sigma$; such sharing imposes unnecessary constraints on those parameters in this use case, and could possibly limit models in expressing diverse $h$. That potential impact can be minimised by just having two separate sets of matrices ($\Sigma_h$, $C_h$) and ($\Sigma_v$, $C_v$), which turns $\eqref{eq_P_a_h_v_h_a}$ and $\eqref{eq_Qn_a_minus_plus}$ into</p>
<p>$$
\begin{aligned}
& P_{\text{a}}(h|v, h_a) = \frac{exp((\overbrace{v^T \Sigma_h^{-0.5} C_h + a^T}^{\hat{S}(v)^T} + Ar(h_a)^T) h)}{\sum_{\hat{h}} exp((\underbrace{v^T \Sigma_h^{-0.5} C_h + a^T}_{\hat{S}(v)^T} + Ar(h_a)^T) \hat{h})}
\end{aligned}
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
& Qn_{\text{a,-+}}(v) = \mu + \Sigma_v^{0.5} C_v h \\
& \phantom{Qn_{\text{a,-+}}(v)} = \mu + \Sigma_v^{0.5} C_v
\begin{bmatrix}
sign(tanh(\hat{S}(v)[1])) \\
\vdots \\
sign(tanh(\hat{S}(v)[m]+\sum_{1 \le k < m} A[m,k] h_a[k]))  \\
\vdots \\
sign(tanh(\hat{S}(v)[K]+\sum_{1 \le k < K} A[K,k] h_a[k]))  \\
\end{bmatrix}
\end{aligned}
$$</p>

</br></br></br></br>

  </article>
</main>

</body>
</html>
