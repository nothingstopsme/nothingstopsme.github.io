

<html data-theme="dark">
  <head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	
  <!-- font awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">

  <link rel="stylesheet" href="/Blogs/static/main/css/classless.css">

  <style>

    @import url('https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&display=swap');


    html[data-theme='dark'] {
      /* foreground   | background color */
      --cfg:   #cdcdcd; --cbg:    #1b1b1b;
      --cdark: #999;    --clight: #343434; /*#333;*/
      --cmed:  #566;
      --clink: #1ad;
      --cemph: #0b9;    --cemphbg: #0b91;


      --font-p: 1.2em/1.7 "Fira Mono", monospace;
      --font-h: .9em/1.5 "Fira Mono", monospace; 
      --font-c: .9em/1.4 "Fira Mono", monospace;

      --width: 54rem;
      --navpos: fixed;
	    --ornament: "";
    }

    article * {
      scroll-margin-top: 9rem; 
    }

    p,
    article table{
      margin-top: 3rem;
    }

    article figure {
      text-align: center;
    }

    article figure img {
      background: #cdcdcd;
    }

    article h3:before {
      left: -0.5rem;
      margin-left: 0rem;
      font-size: 1.5rem;
    }

    article h2:before {
      left: -0.5rem;
      margin-left: -2rem;
      font-size: 1.5rem;
    }

    article h2.reference-title:before {
      display: none;
    }

    article ul {
      overflow: hidden;
    }

    

    a[href] {
      text-decoration: none;
    }

    

    nav ul:not(:first-child) li {
      padding: 0;
      margin: 0;
    }


    nav input[type="checkbox"]:hover + label, 
    nav input[type="checkbox"]:focus + label, 
    nav a.nav-active,
    nav a:hover, 
    nav a:focus {
      background: black;
    }


    nav li > a {
      display: block;
    }

    nav > span {
      height: inherit;
      background: inherit;
    }

    nav > span > input[type="checkbox"] {
      width: 0;
    }

    nav > span > input[type="checkbox"] + label { 
      color: var(--clink); cursor: pointer; 
    }


    nav > span > ul {
      background: inherit;
      display: inline-block;
      width: auto;
      margin: 0;
      padding: 0;
    }

    nav > span > input[type="checkbox"] + label {
      height: inherit;
      padding: 1rem 0.6rem;
    }

    nav > span > ul > li {
      height: 4rem;
      display: inline-block;
    }

    nav > span > ul > li > a {
      height: inherit;
      padding: 1rem 0.6rem;
    }

   
    nav > span.left-menu {
      float: left;
    }

    nav > span.left-menu > input[type="checkbox"] + label {
      float: left;
      display: none;
    }

    nav > span.left-menu > input[type="checkbox"] + label + ul {
      float: left;
      clear: left;
      
    }
    
    nav > span.left-menu > input[type="checkbox"] + label + ul > li{
      float: left;
    }

     
    nav > span.right-menu {
      float: right;
    }


    nav > span.right-menu > input[type="checkbox"] + label {
      float: right;
      display: inline-block;
    }

    nav > span.right-menu > input[type="checkbox"] + label + ul {
      display: none;
      float: right;
      clear: right;
      overflow-y: auto;
      max-height: 50vh;

      border: var(--border);
      border-radius: 4px;

    }

    nav > span.right-menu > input[type="checkbox"] + label + ul > li {
      display: block;
    }

    nav > span.right-menu > input[type="checkbox"]:hover + label + ul,
    nav > span.right-menu > input[type="checkbox"]:focus + label + ul,
    nav > span.right-menu > input[type="checkbox"] ~ ul:hover {
        display: inline-block;
    }

   
    body>nav {
      
      left: max(calc(50vw - var(--width)/2), 0vw);
      right: max(calc(50vw - var(--width)/2), 0vw);
      width: auto;
      height: 4rem;
      box-shadow: none;
      z-index: 100;
      overflow-y: visible;

    }

		article div.post-tags span {
			white-space: nowrap;
		}

    article div.post-tags span:not(:first-child):before
    {
			content: "|";
    }
  
   
    @media (max-width: 40rem) 
    {
      nav > span.left-menu > input[type="checkbox"] + label + ul > li {
        float: none;
        display: block;
      }
   
      nav > span.left-menu > input[type="checkbox"] + label {
        display: inline-block;
      }
      
      nav > span.left-menu > input[type="checkbox"] + label + ul {
        display: none;
      }

      nav > span.left-menu > input[type="checkbox"]:hover + label + ul,
      nav > span.left-menu > input[type="checkbox"]:focus + label + ul,
      nav > span.left-menu > input[type="checkbox"] + label + ul:hover {
        display: inline-block;
     

        overflow-y: auto;
        max-height: 50vh;

        border: var(--border);
        border-radius: 4px;
      }
          

    }

  </style>

  <title>
From VAE, Hierarchical VAE, to Variational Diffusion Model
</title>
  
	
  </head>

<body style="margin-top: 0; padding-top: 0;">
<nav>
  <span class="left-menu">
  <input type="checkbox" id="nav-menu" />
  <label for="nav-menu">
  &nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;
  </label>

  <!--<a href="#">&nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;</a>-->
  <ul>
    <!--<li class="menu-hamburger">
    </li>-->
    
      
          
          <li>
            <a href="/Blogs/archives/"
            
            

            >Archives</a>                            
          </li>
      
          
          <li>
            <a href="/Blogs/tags/"
            
            

            >Tags</a>                            
          </li>
      
    
  </ul>
  </span>

  
  

</nav>

<main style='margin-top: 4rem; padding: 0 2rem; overflow-y: hidden;'>
  <article>
      
<script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    /*inlineMath: [['$','$'], ['\\(','\\)']],*/
    inlineMath: [['$','$']],
    processEscapes: true},
    jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
    extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
    TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
    equationNumbers: {
    autoNumber: "AMS"
    }
  }
});
</script>


<h1>From <abbr title="Variational AutoEncoder">VAE</abbr>, Hierarchical <abbr title="Variational AutoEncoder">VAE</abbr>, to Variational Diffusion Model</h1>


<div class="post-tags">
  
  <span>
  <i class="fas fa-calendar"></i>&nbsp;<time>20.Jan.2023</time>
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;Probability
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;MachineLearning
  </span>
  
  
</div>



<p><strong>V</strong>ariational <strong>D</strong>iffusion <strong>M</strong>odels, a relatively new design of generative models, have demonstrated its competitiveness against other state-of-the-art methods (and in some cases outperformed). By the look of its name, one (OK, I mean me) might think it must derive from a ground-breaking and sophisticated mathematical theory in order to grant it such power; but a tutorial I read recently has completely broken down my prejudice: it can be interpreted as simple as a form of <strong>H</strong>ierarchical <abbr title="Variational AutoEncoder">VAE</abbr>. This post will cover that particular perspective which I think make <abbr title="Variational Diffusion Model">VDM</abbr> become really straightforward and understandable to me.</p>
<h2 id="VAE_and_HVAE"><abbr title="Variational AutoEncoder">VAE</abbr> and <abbr title="Hierarchical Variational AutoEncoder">HVAE</abbr></h2>
<p>Starting with these two types of models, the main objective of <abbr title="Variational AutoEncoder">VAE</abbr> is to estimate data distribution on the principle of maximum likelihood: given a data set $D_X$ of the size $\|D_X\|$, we want to find a distribution $p(x)$ that can maximise $\sum_{d_x \in D_X} \frac{1}{|D_X|} log\;p(x = d_x)$. However, for real-world data $p(x)$ is usually unknown or intractable, and thus $log\;p(x)$ cannot be evaluated, let alone to solve that maximisation problem. To work around that, the inventors of <abbr title="Variational AutoEncoder">VAE</abbr> derived the <strong>E</strong>vidence <strong>L</strong>ower <strong>BO</strong>und of $log\;p(x)$ by introducing a latent variable $z$ (note that $x$ is the observed variable) and a auxiliary distribution $q(z|x)$:</p>
<p>$$
\begin{align}
& log\;p(x) \nonumber \\
& = log \big( \int q(z|x) \frac{p(x, z)}{q(z|x)} dz \big) \nonumber \\
& \ge \int q(z|x) log \big( \frac{p(x,z)}{q(z|x)} \big) dz \,\,\, \text{(Applying Jensenâ€™s Inequality)} \nonumber \\
& = E_{g} [ log \; p(x|z) ] + E_{g} [ log \big( \frac{p(z)}{q(z|x)} \big) ] \nonumber \\
& = E_{g} [ log \; p(x|z) ] - KL[ q(z|x) \| p(z) ]  \label{eq:VAE_ELBO_1} \\
& = E_{g} [ log \big( \frac{p(x, z)}{p(z|x)} \big) ] + E_{g} [ log \big( \frac{p(z|x)}{q(z|x)} \big) ] \nonumber \\
& = E_{g} [ log \; p(x) ] - KL[ q(z|x) \| p(z|x) ]  \nonumber \\
& = log \; p(x) - KL[ q(z|x) \| p(z|x) ]  \label{eq:VAE_ELBO_2}
\end{align}
$$</p>
<p>The two equivalent forms of <abbr title="Evidence Lower BOund">ELBO</abbr> ($\eqref{eq:VAE_ELBO_1}$ and $\eqref{eq:VAE_ELBO_2}$) reveal the fact that the maximisation of $\eqref{eq:VAE_ELBO_1}$ will lead to the maximisation of $log\;p(x)$ and the minimisation of $KL[ q(z|x) \| p(z|x) ]$; and when $KL[ q(z|x) \| p(z|x) ] = 0$, i.e. $q(z|x)$ and $p(z|x)$ are identical, maximising $\eqref{eq:VAE_ELBO_1}$ is just the same as maximising $log\;p(x)$. That is the reason why $q(z|x)$ of this particular form is brought in as the auxiliary distribution in the first place, because we want that $\eqref{eq:VAE_ELBO_2}$ as close to $log\;p(x)$ as possible, so that one can use $\eqref{eq:VAE_ELBO_1}$ which only involves $q(z|x)$, $p(x|z)$, and $p(z)$ as a proxy objective to indirectly search a $p$ that can be the (approximate) solution to the original maximisation problem. Being able to use these three alternative distributions has the advantage in that, the two conditionals can be defined by function mappings (such as neural networks) taking given conditions as input to produce the description of output distributions, while $p(z)$ might be of any form as long as it matches how model designers think the latent variable should be distributed. Therefore, a typical <abbr title="Variational AutoEncoder">VAE</abbr> normally contains one mapping (as an encoder) to describe $q(z|x)$; another (as a decoder) to describe $p(x|z)$, and a tractable distribution (as a prior) choosen by model designers for $p(z)$. Note that if both $p(x|z)$ and $p(z)$ are set up to be some distributions from which we know how to sample, then we can actually generate new data points with <abbr title="Variational AutoEncoder">VAE</abbr> through first sampling $z$ and then given the instance of $z$ sampling $x$. That property opens up the potential of <abbr title="Variational AutoEncoder">VAE</abbr> being a generative model.</p>
<p id="included_image_Fig1">
<figure><img alt="graphical_model_VAE.png" src="/Blogs/static/main/images/Variational-Diffusion-Model/graphical_model_VAE.png" /><figcaption>The graphical model for <abbr title="Variational AutoEncoder">VAE</abbr></figcaption>
</figure></p>
<p>As for <abbr title="Hierarchical Variational AutoEncoder">HVAE</abbr>, to put it simply it is a generalised form of <abbr title="Variational AutoEncoder">VAE</abbr> which allows multiple latent variables; hence for latent variables $z_1 \dots z_T$, the <abbr title="Evidence Lower BOund">ELBO</abbr> becomes:</p>
<p>$$
\begin{equation}
\begin{aligned}
& log\;p(x) \\
& = log \big( \int q(z_1, \cdots, z_T|x) \frac{p(x, z_1, \cdots, z_T)}{q(z_1, \cdots, z_T|x)} dz_1 \dots dz_T \big) \\
& \ge \int q(z_1, \cdots, z_T |x) log \big( \frac{p(x,z_1, \cdots, z_T)}{q(z_1, \cdots, z_T|x)} \big) dz_1 \dots dz_T \\
& = \int q(z_1 |x) \prod_{i=2}^T q(z_i | z_{ < i}, x)  \\
& \phantom{= \int} log \big( \frac{p(x | z_1) \big( \prod_{j=1}^{T-1} p(z_j | z_{ > j}) \big) p(z_T)}{q(z_1 |x) \prod_{i=2}^T q(z_i | z_{ < i}, x)} \big) dz_1 \dots dz_T \\
\end{aligned}
\label{eq:HVAE_ELBO}
\end{equation}
$$</p>
<p>where the conditional dependency across latent and observed variables shows a hierarchical structure, as illustrated in <a href="#included_image_Fig2">Figure 2</a>. Likewise, we can use $p(x | z_1) \big( \prod_{j=1}^{T-1} p(z_j | z_{>j}) \big) p(z_T)$ to generate new data samples once these conditionals that maximise $\eqref{eq:HVAE_ELBO}$ are obtained.</p>
<p id="included_image_Fig2">
<figure><img alt="graphical_model_HVAE_4nodes.png" src="/Blogs/static/main/images/Variational-Diffusion-Model/graphical_model_HVAE_4nodes.png" /><figcaption>The graphical model for <abbr title="Hierarchical Variational AutoEncoder">HVAE</abbr> with T = 3</figcaption>
</figure></p>
<h2 id="Complexity_Reduction">Reducing the Complexity of <abbr title="Hierarchical Variational AutoEncoder">HVAE</abbr></h2>
<p>The fully conditional dependency formulated in $\eqref{eq:HVAE_ELBO}$ has the highest capacity for representing the joints, but it also means the cost of modelling grows exponentially as the number of latent variables increases. One way to alleviate these overheads is to introduce the assumption of conditional independency, such as (for brevity, $z_0$ is substituted for $x$):</p>
<p>$$
\begin{equation}
\begin{cases}
& \begin{aligned}
& p(x, z_1, \cdots, z_T) = p(z_0, z_1, \cdots, z_T) \\
& = p(z_0 | z_1) \big( \prod_{i = 2}^{T} p(z_{i-1}|z_i) \big) p(z_T)
\end{aligned} \\
& \begin{aligned}
& q(z_1, \cdots, z_T|x) = q(z_1, \cdots, z_T| z_0) \\
& = q(z_1 | z_0) \prod_{i = 2}^{T} q(z_i|z_{i-1}, z_0) \\
& = q(z_1 | z_0) \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0) \frac{q(z_i|z_0)}{q(z_{i-1}|z_0)} \\
& = q(z_1 | z_0) \big( \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0) \big) \frac{q(z_T|z_0)}{q(z_1|z_0)} \\
& = q(z_T | z_0) \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0)
\end{aligned}
\end{cases}
\label{eq:pq_decomposition}
\end{equation}
$$</p>
<p id="included_image_Fig3">
<figure><img alt="graphical_model_HVAE_4nodes_cind.png" src="/Blogs/static/main/images/Variational-Diffusion-Model/graphical_model_HVAE_4nodes_cind.png" /><figcaption>The graphical model for <abbr title="Hierarchical Variational AutoEncoder">HVAE</abbr> with T = 3, under the assumption of the conditional independency set out in $\eqref{eq:pq_decomposition}$</figcaption>
</figure></p>
<p>Then the decomposition described in $\eqref{eq:pq_decomposition}$ gives rise to a different <abbr title="Evidence Lower BOund">ELBO</abbr>:</p>
<p>$$
\begin{align}
& log\;p(x) \nonumber \\
& = log\;p(z_0) \nonumber \\
& \ge \int q(z_1, \cdots, z_T |z_0) log \big( \frac{p(z_0,z_1, \cdots, z_T)}{q(z_1, \cdots, z_T|z_0)} \big) dz_1 \dots dz_T \nonumber \\
& = \int q(z_1, \cdots, z_T |z_0) \nonumber \\
& \phantom{= \int}log \big( \frac{p(z_0 | z_1) \big( \prod_{i = 2}^{T} p(z_{i-1}|z_i) \big) p(z_T)}{q(z_T | z_0) \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0)} \big) dz_1 \dots dz_T \nonumber \\
& = E_{q(z_1, \cdots, z_T |z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} + E_{q(z_1, \cdots, z_T |z_0)} [ log \big( \frac{p(z_T)}{q(z_T|z_0)} \big) ] \nonumber \\
& \phantom{=} + E_{q(z_1, \cdots, z_T |z_0)} [ log \big( \prod_{i=2}^T \frac{p(z_{i-1}|z_i)}{q(z_{i-1}|z_i, z_0)} \big)] \nonumber \\
& = E_{q(z_1 | z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} + E_{q(z_T |z_0)} [ log \big( \frac{p(z_T)}{q(z_T|z_0)} \big) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_T | z_0) \prod_{j = 2, j \ne i}^{T} q(z_{j-1}| z_j, z_0)} \big[ E_{q(z_{i-1}|z_i, z_0)}[ log \big( \frac{p(z_{i-1}|z_i)}{q(z_{i-1}|z_i, z_0)} \big) ] \big] \nonumber \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_T | z_0) \prod_{j = 2, j \ne i}^{T} q(z_{j-1}| z_j, z_0)} \big[ \underbrace{KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ]}_{\text{constant with respect to } z_{i-1}} \big] \nonumber \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_1, \cdots, z_T |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big] \nonumber \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_i |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big] \label{eq:HVAE_CIND_ELBO} \\
\end{align}
$$</p>
<p>Notice that when $T=1$, $\eqref{eq:HVAE_CIND_ELBO}$ falls back to:</p>
<p>$$
\begin{aligned}
& E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \\
& \phantom{=} - KL[q(z_1|z_0) \| p(z_1) ] \\
& \phantom{=} - \underbrace{\sum_{i=2}^{1} E_{q(z_i | z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big]}_{\text{summation over nothing}} \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] - KL[q(z_1|z_0) \| p(z_1)] \\
& = E_{q(z_1|x)} [ log \; p(x|z_1) ] - KL[q(z_1|x) \| p(z_1)]
\end{aligned}
$$</p>
<p>which is exactly the same as $\eqref{eq:VAE_ELBO_1}$ of <abbr title="Variational AutoEncoder">VAE</abbr>; thus this summation $\sum_{i=2}^T E_{q(z_i |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big]$ can be viewed as additional criteria to makes sure that for every newly introduced latent variable $z_{i-1}$, its conditionals ($q(z_{i-1}|z_i, z_0)$ and $p(z_{i-1}|z_i)$) found for maximising $\eqref{eq:HVAE_CIND_ELBO}$ are similar on average.</p>
<h2><abbr title="Variational Diffusion Model">VDM</abbr></h2>
<p>The key of <abbr title="Variational Diffusion Model">VDM</abbr> is the concept of <a href="https://en.wikipedia.org/wiki/Diffusion">diffusion processes</a>, in which the substance of interest gradually and randomly spreads out. Just like particle diffusion in physics, in <abbr title="Variational Diffusion Model">VDM</abbr> data representation is set to be the one undergoing a diffusion process over time, and at the end of the process, one would expect it becomes completely randomised; in other words, the diffusion process of <abbr title="Variational Diffusion Model">VDM</abbr> will turn data into noise.</p>
<p>For continuous data, such a diffusion process can be formulated as the addition of various degrees of standard gaussian noise to the observed variable $z_0$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& z_i \\
& = \sqrt{\bar{\alpha}_i} z_0 + \sqrt{1 - \bar{\alpha}_i} \epsilon_i \\
& \sim N(\sqrt{\bar{\alpha}_i} z_0, (1 - \bar{\alpha}_i) I ) \\
& = q(z_i | z_0)
\end{aligned}
\label{eq:gaussian_noise_addition}
\end{equation}
$$</p>
<p>where $z_i$ is the random variable for the outcomes at the $i$-th discrete time step in this diffusion process; $\epsilon_i \sim N(0, I)$; and $\bar{\alpha}_i$ is its parameter controlling the strength of noise being added such that</p>
<ol>
<li>$\bar{\alpha}_i < \bar{\alpha}_j$ for every $j < i$. This makes the distribution of $z_i$ closer to $N(0, I)$ than others preceding it.</li>
<li>Given a total of $T$ time steps, $\bar{\alpha}_T = 0$ and therefore $z_T \sim N(0, I)$ (the results at the last step follows a standard guassian).</li>
</ol>
<p>OK, now we have this particular diffusion process defined, and we know how to directly sample at each time step; but what is the use of it? Well, smart researchers come up with this idea: if $p(z_i | z_{>i})$ for all $0 \le i < T$ and $p(z_T)$ are known, one can in a sense create a reverse process which generates data (outcomes of $z_0$) from samples of $z_T$. To that end, we want to find those conditionals and marginal which are consistent with $\eqref{eq:gaussian_noise_addition}$ and the data distribution, and the consistency is imposed by the following KL divergence (given $q(z_0) = \frac{1}{\| D_x \|}$ being the empirical data distribution of a data set $D_x$, and $q(z_1, \cdots, z_T | z_0) $ being the joint distribution of all $z_i$ over the course of the diffusion process):</p>
<p>$$
\begin{equation}
\begin{aligned}
& KL[q \| p]  \\
& = -\int q(z_0, z_1, \cdots, z_T)  \\
& \phantom{=} log \big(\frac{p(z_0, z_1, \cdots, z_T)}{q(z_0, z_1, \cdots, z_T)} \big) dz_0 \dots dz_T  \\
& = -\int q(z_1, \cdots, z_T | z_0) q(z_0)  \\
& \phantom{=} log \big(\frac{\prod_{i=0}^{T-1} p(z_i | z_{>i}) p(z_T)}{q(z_1, \cdots, z_T | z_0) q(z_0)} \big) dz_0 \dots dz_T   \\
& = -E_{q(z_0)}[\int q(z_1, \cdots, z_T | z_0)  \\
& \phantom{= E_{q(z_0)}[\int} log \big(\frac{\prod_{i=0}^{T-1} p(z_i | z_{>i}) p(z_T)}{q(z_1, \cdots, z_T | z_0)} \big) dz_1 \dots dz_T]  \\
& \phantom{=} + E_{q(z_0)} [ log \big( q(z_0) \big) ]  \\
& = -E_{q(z_0)} [\int q(z_1, \cdots, z_T | z_0)  \\
& \phantom{= E_{q(z_0)} [ \int} log \big(\frac{\prod_{i=0}^{T-1} p(z_i | z_{>i}) p(z_T)}{q(z_1, \cdots, z_T | z_0)} \big) dz_1 \dots dz_T ]  \\
& \phantom{=} - entropy(q(z_0))
\end{aligned}
\label{eq:KL_for_consistency_1}
\end{equation}
$$</p>
<p>Since $entropy(q(z_0))$ is constant, to impose this criterion, i.e. to minimise $KL[q \| p]$, is equivalent to maximise:</p>
<p>$$
\require{cancel}
\begin{equation}
\begin{aligned}
& E_{q(z_0)} [ \int q(z_1, \cdots, z_T | z_0) \\
& \phantom{= E_{q(z_0)} [ \int} log \big(\frac{\prod_{i=0}^{T-1} p(z_i | z_{>i}) p(z_T)}{q(z_1, \cdots, z_T | z_0)} \big) dz_1 \dots dz_T ] \\
& = E_{q(z_0)} [ \int q(z_1, \cdots, z_T | z_0) \\
& \phantom{= E_{q(z_0)} [ \int} log \big(\frac{p(z_0, z_1, \cdots, z_T)}{q(z_1, \cdots, z_T | z_0)} \big) dz_1 \dots dz_T ] \\
& = \sum_{d_x \in D_x} \frac{1}{| D_x |} \Big( \int q(z_1, \cdots, z_T | z_0 = d_x) \\
& \phantom{= \sum_{d_x \in D_x} \frac{1}{| D_x |} \Big( \int} log \big(\frac{p(z_0 = d_x, z_1, \cdots, z_T)}{q(z_1, \cdots, z_T | z_0 = d_x)} \big) dz_1 \dots dz_T \Big) \\
& \le \sum_{d_x \in D_x} \frac{1}{| D_x |} \Big( log \big( \int p(z_0 = d_x, z_1, \cdots, z_T) dz_1 \dots dz_T \big) \Big) \\
& = \sum_{d_x \in D_x} \frac{1}{| D_x |} log \big( p(z_0 = d_x) \big) \\
\end{aligned}
\label{eq:KL_for_consistency_2}
\end{equation}
$$</p>
<p>which is just the same maximisation target as $\eqref{eq:HVAE_ELBO}$, except that the expectation over the data distribution has also been included in $\eqref{eq:KL_for_consistency_2}$; therefore, from the view of <abbr title="Hierarchical Variational AutoEncoder">HVAE</abbr> modelling, <abbr title="Variational Diffusion Model">VDM</abbr> can be interpreted as a <abbr title="Hierarchical Variational AutoEncoder">HVAE</abbr> given a fixed auxiliary distribution $q$ derived from the diffusion process according to $\eqref{eq:gaussian_noise_addition}$, and according to the trick of complexity reduction discussed in this <a href="#Complexity_Reduction">section</a>, its objective might be formulated as $\sum_{d_x \in D_x} \frac{1}{\| D_x \|} Obj_{\text{VDM}}(z_0 = d_x)$, where</p>
<p>$$
\begin{equation}
\begin{aligned}
& Obj_{\text{VDM}}(z_0) \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_i |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big] \\
\end{aligned}
\label{eq:Obj_VDM}
\end{equation}
$$</p>
<p>Note that $\eqref{eq:Obj_VDM}$ goes particularly well with our diffusion process, because:</p>
<ul>
<li>We can easily sample from $q(z_i | z_0)$ for any $1 \le i \le T$, so in practice all expectations appearing in $\eqref{eq:Obj_VDM}$ can be estimated with low variance via <em>Monte Carlo estimate</em>.</li>
<li>When choosing the distribution of $p(z_T)$ (just as picking the prior in <abbr title="Variational AutoEncoder">VAE</abbr>), if $p(z_T)$ is set to be a standard gaussian, then $KL [ q(z_T|z_0) \| p(z_T) ] = 0$ (due to the fact that $q(z_T|z_0)$ is also $N(0, I)$)</li>
</ul>
<p>Nonetheless, there are still three missing pieces before we can use $\eqref{eq:Obj_VDM}$ to start to learn a <abbr title="Variational Diffusion Model">VDM</abbr>:</p>
<h3>What distribution is $q(z_{i-1}|z_i, z_0)$?</h3>
<p>It is indeed obscure as we can not read it out directly from $\eqref{eq:gaussian_noise_addition}$, but research has demonstrated that, given $q(z_T | z_0) = N(\sqrt{\bar{\alpha}_T} z_0, (1 - \bar{\alpha}_T) I )$, the following gaussian matches our diffusion process</p>
<p>$$
\begin{equation}
\begin{aligned}
& q(z_{i-1} |z_i, z_0) \\
& = N(\underbrace{\sqrt{\bar{\alpha}_{i-1}} z_0 + \sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2} \frac{z_i - \sqrt{\bar{\alpha}_i} z_0}{\sqrt{1 - \bar{\alpha}_i}}}_{M_i}, \sigma_i^2 I)
\end{aligned}
\label{eq:q_z_i_1_given_z_i_z_0}
\end{equation}
$$</p>
<p>with $\sigma_i^2$ being the magnitude of the isotropic variance at the time step $i$. To show that is true, assuming that $q(z_i | z_0) = N(\sqrt{\bar{\alpha}_i} z_0, (1 - \bar{\alpha}_i) I )$, then</p>
<p>$$
\begin{equation}
\begin{aligned}
& q(z_{i-1} | z_0) \\
& = \int q(z_{i-1} |z_i, z_0) q(z_i | z_0) dz_i \\
& \propto \int exp \Big(-\frac{1}{2} \big( \frac{z_{i-1}^T z_{i-1}}{\sigma_i^2} - 2 \frac{z_{i-1}^T M_i}{\sigma_i^2} + \frac{M_i^T M_i}{\sigma_i^2} \big) \Big) \\
& \phantom{\propto \int } exp\Big(-\frac{1}{2} \big( \frac{z_i^T z_i}{1 - \bar{\alpha}_i} - 2 \frac{\sqrt{\bar{\alpha}_i} z_i^T z_0 }{1 - \bar{\alpha}_i}  +  \frac{\bar{\alpha}_i z_0^T z_0}{1 - \bar{\alpha}_i} \big) \Big) dz_i \\
& = \underbrace{f(z_0, z_{i-1})}_{\text{terms not involving } z_i} \int \underbrace{g(z_0, z_{i-1}, z_i)}_{\text{terms involving } z_i} dz_i
\end{aligned}
\label{eq:q_z_minus_1_given_z_0}
\end{equation}
$$</p>
<p>Since</p>
<p>$$
\begin{aligned}
& \frac{z_{i-1}^T M_i}{\sigma_i^2} \\
& = \frac{\sqrt{\bar{\alpha}_{i-1}}}{\sigma_i^2} z_{i-1}^T z_0 + \frac{\sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} (z_{i-1}^T z_i - \sqrt{\bar{\alpha}_i} z_{i-1}^T z_0)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
& \frac{M_i^T M_i}{\sigma_i^2} \\
& = \frac{\bar{\alpha}_{i-1}}{\sigma_i^2} z_0^T z_0 + 2 \frac{\sqrt{\bar{\alpha}_{i-1}} \sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} (z_0^T z_i - \sqrt{\bar{\alpha}_i} z_0^T z_0)\\
& \phantom{=} \frac{1-\bar{\alpha}_{i-1} - \sigma_i^2}{\sigma_i^2 (1 - \bar{\alpha}_i)} (z_i^T z_i - 2\sqrt{\bar{\alpha}_i} z_i^T z_0 + \bar{\alpha}_i z_0^T z_0)
\end{aligned}
$$</p>
<p>therefore</p>
<p>$$
\begin{equation}
\begin{aligned}
& g(z_0, z_{i-1}, z_i) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1}{1 - \bar{\alpha}_i} z_i^T z_i + \frac{1-\bar{\alpha}_{i-1} - \sigma_i^2}{\sigma_i^2 (1 - \bar{\alpha}_i)} z_i^T z_i \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{ \sqrt{\bar{\alpha}_i}}{1 - \bar{\alpha}_i} z_i^T z_0 \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} + 2 \frac{\sqrt{\bar{\alpha}_{i-1}} \sqrt{1-\bar    {\alpha}_{i-1} - \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} z_i^T z_0 \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{(1-\bar{\alpha}_{i-1} - \sigma_i^2) \sqrt{\bar{\alpha}_i}}{\sigma_i^2 (1 - \bar{\alpha}_i)}) z_i^T z_0 \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{\sqrt{1-\bar{\alpha}_{i-1} -     \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} z_i^T z_{i-1} \\
& \phantom{= exp} \big) \Big) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} z_i^T z_i \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} z_i^T \nu(z_0, z_{i-1}) \big) \Big) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \| z_i - \nu(z_0, z_{i-1}) \|^2_2 \big) \Big) \\
& \phantom{= exp} exp\Big(\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \nu(z_0, z_{i-1})^T \nu(z_0, z_{i-1}) \big) \Big)
\end{aligned}
\label{eq:g_of_z_0_z_i_minus_1_z_i}
\end{equation}
$$</p>
<p>where</p>
<p>$$
\begin{aligned}
& \nu(z_0, z_{i-1}) \\
& = (\sqrt{\bar{\alpha}_i} - \frac{\sqrt{(1-\bar{\alpha}_i)\bar{\alpha}_{i-1}(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{1-\bar{\alpha}_{i-1}}) z_0 \\
& \phantom{=} + \frac{\sqrt{(1-\bar{\alpha}_i)(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{1-\bar{\alpha}_{i-1}} z_{i-1}
\end{aligned}
$$</p>
<p>$\eqref{eq:g_of_z_0_z_i_minus_1_z_i}$ suggests that (based on the form of the normaliser for any multivariate gaussian) $\int g(z_0, z_{i-1}, z_i) dz_i$ is just $\sqrt{det(2 \pi \frac{\sigma_i^2 (1 - \bar{\alpha}_i)}{1 - \bar{\alpha}_{i-1}}I)} exp\Big(\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \nu(z_0, z_{i-1})^T \nu(z_0, z_{i-1}) \big) \Big)$, which allows us to further simplify $\eqref{eq:q_z_minus_1_given_z_0}$:</p>
<p>$$
\begin{aligned}
& q(z_{i-1} | z_0) \\
& \propto f(z_0, z_{i-1}) \int g(z_0, z_{i-1}, z_i) dz_i \\
& \propto exp\Big(-\frac{1}{2}\big( \frac{z_{i-1}^T z_{i-1}}{\sigma_i^2} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - 2 \frac{\sqrt{\bar{\alpha}_{i-1}}}{\sigma_i^2} z_{i-1}^T z_0 \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} + 2 \frac{\sqrt{\bar{\alpha}_i(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{\sigma_i^2 \sqrt{1-\bar{\alpha}_i}} z_{i-1}^T z_0 \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \nu(z_0, z_{i-1})^T \nu(z_0, z_{i-1}) \big) \Big) \\
& \propto exp\Big(-\frac{1}{2}\big( \frac{z_{i-1}^T z_{i-1}}{\sigma_i^2} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - 2 \frac{\sqrt{\bar{\alpha}_{i-1}}}{\sigma_i^2} z_{i-1}^T z_0 \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} \cancel {+ 2 \frac{\sqrt{\bar{\alpha}_i(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{\sigma_i^2 \sqrt{1-\bar{\alpha}_i}} z_{i-1}^T z_0} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - \frac{1-\bar{\alpha}_{i-1}-\sigma_i^2}{\sigma_i^2 (1-\bar{\alpha}_{i-1})} z_{i-1}^T z_{z_i} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} \cancel{- 2 \frac{\sqrt{\bar{\alpha}_i (1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{\sigma_i^2 \sqrt{1-\bar{\alpha}_{i}}} z_{i-1}^T z_0} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} + 2 \frac{\sqrt{\bar{\alpha}_{i-1}} (1-\bar{\alpha}_{i-1}-\sigma_i^2)}{\sigma_i^2 (1-\bar{\alpha}_{i-1})} z_{i-1}^T z_0 \big) \Big) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1}{1-\bar{\alpha}_{i-1}} z_{i-1}^T z_{i-1} - 2 \frac{\sqrt{\bar{\alpha}_{i-1}}}{1-\bar{\alpha}_{i-1}} z_{i-1}^T z_0 \big) \Big) \\
& \propto exp\Big(-\frac{1}{2}\big( \frac{\|z_{i-1} - \sqrt{\bar{\alpha_{i-1}}}z_0 \|^2_2}{1-\bar{\alpha_{i-1}}} \big) \Big) \\
& \propto N(\sqrt{\bar{\alpha_{i-1}}}z_0, (1-\bar{\alpha_{i-1}}) I)
\end{aligned}
$$</p>
<p>Hence by induction, all $q(z_1 | z_0), \cdots, q(z_T | z_0)$ match the diffusion process defined by $\eqref{eq:gaussian_noise_addition}$.</p>
<h3>How to parameterise $p(z_{i-1}|z_i)$ for $2 \le i \le T$?</h3>
<p>The answer to this one becomes more obvious after $\eqref{eq:q_z_i_1_given_z_i_z_0}$ is established: since we want to minimise $KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ]$ for maximising $\eqref{eq:Obj_VDM}$, the distribution of $p(z_{i-1}|z_i)$ should be able to be shaped as close to $q(z_{i-1}|z_i, z_0)$ as possible. Note that $q(z_{i-1}|z_i, z_0)$ itself is actually a guassian parameterised by a fixed $\sigma_i^2$ and a variable $M_i$ which is in effect a function of $z_0, z_i$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& M_i(z_0, z_i) \\
& = \sqrt{\bar{\alpha}_{i-1}} z_0 + \sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2} \frac{z_i - \sqrt{\bar{\alpha}_i} z_0}{\sqrt{1 - \bar{\alpha}_i}} \\
& = \underbrace{\frac{\sqrt{\bar{\alpha}_{i-1}(1-\bar{\alpha}_i)} - \sqrt{\bar{\alpha}_i (1 - \bar{\alpha}_{i-1}- \sigma_i^2)}}{\sqrt{1-\bar{\alpha}_i}}}_{w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} z_0 \\
& \phantom{=} + \underbrace{\frac{\sqrt{1 - \bar{\alpha}_{i-1} - \sigma_i^2}}{\sqrt{1 - \bar{\alpha}_i}}}_{w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} z_i
\end{aligned}
\label{eq:M_i_z_0_z_i}
\end{equation}
$$</p>
<p>So why not parameterise $p(z_{i-1}|z_i)$ in the same way as a guassian?</p>
<p>$$
\begin{equation}
\begin{aligned}
& p(z_{i-1}|z_i) \\
& = N(w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) f(i, z_i) + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i, \sigma_i^2 I)
\end{aligned}
\label{eq:p_z_i_minus_1_given_z_i}
\end{equation}
$$</p>
<p>with $f$ being the function to be learnt, shared across time steps, taking time index $i$ and $z_i$ as input to in a sense "predict" $z_0$ (as $p(z_{i-1}|z_i)$ does not condition on $z_0$). In addition to similarity, another reason of $\eqref{eq:p_z_i_minus_1_given_z_i}$ being a good choice is that, the target KL will be computed over two gaussians, and thus we have a closed form for it:</p>
<p>$$
\begin{equation}
\begin{aligned}
& KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \\
& = \frac{1}{2} \Big( log \big(\frac{det(\sigma_i^2 I)}{det(\sigma_i^2 I)} \big) - trace(I) + trace(\frac{1}{\sigma_i^2} I \sigma_i^2 I) \\
& \phantom{= \frac{1}{2} \Big(} + \frac{1}{\sigma_i^2} \|(w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) f(i, z_i) + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i) \\
& \phantom{= \frac{1}{2} \Big( + \frac{1}{\sigma_i^2} \|} - (w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_0 + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i)\|^2_2 \Big) \\
& = \frac{w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)^2}{2 \sigma_i^2} \| f(i, z_i) - z_0 \|_2^2
\end{aligned}
\label{eq:KL_q_p_simplified}
\end{equation}
$$</p>
<p>It is also worth mentioning that there is an equivalent formulation where $\eqref{eq:M_i_z_0_z_i}$, $\eqref{eq:p_z_i_minus_1_given_z_i}$, and $\eqref{eq:KL_q_p_simplified}$ are rewritten using $z_0 = \frac{z_i - \sqrt{1-\bar{\alpha}_i}\epsilon_i}{\sqrt{\bar{\alpha}_i}}$, the fact derived from $\eqref{eq:gaussian_noise_addition}$, as follows:</p>
<p>$$
\begin{equation}
\begin{aligned}
& M_i(z_0, z_i) \\
& = w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) \frac{z_i - \sqrt{1-\bar{\alpha}_i}\epsilon_i}{\sqrt{\bar{\alpha}_i}} + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i \\
& = \underbrace{\frac{-w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)\sqrt{1-\bar{\alpha}_i}}{\sqrt{\bar{\alpha}_i}}}_{\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} \epsilon_i + \underbrace{\frac{w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i)}{\sqrt{\bar{\alpha}_i}}}_{\hat{w}_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} z_i \\
& \; \\
& p(z_{i-1}|z_i) \\
& = N(\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) e(i, z_i) + \hat{w}_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i, \sigma_i^2 I) \\
& \; \\
& KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \\
& = \frac{\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)^2}{2 \sigma_i^2} \| e(i, z_i) - \epsilon_i \|_2^2
\end{aligned}
\label{eq:equivalent}
\end{equation}
$$</p>
<p>Compared with $f(i, z_i)$ from the original form, the function $e(i, z_i)$ in the equivalent has a different meaning in that it is to estimate the noise present in $z_i$, and this change reportedly results in a better model.</p>
<h3>How about $p(z_0|z_1)$?</h3>
<p>Now that we have $f(i, z_i)$/$e(i, z_i)$ defined, which can be used to estimate $z_0$, it seems intuitively straightforward to share the same function and set $p(z_0|z_1)$ to:</p>
<p>$$
\begin{equation}
\begin{aligned}
& p(z_0|z_1) \\
& = N(f(1, z_1), \sigma^2 I) \\
& = N(\frac{z_1 - \sqrt{1-\bar{\alpha}_1}e(1, z_1)}{\sqrt{\bar{\alpha}_1}}, \sigma_1^2 I)
\end{aligned}
\label{eq:p_z_0_z_1}
\end{equation}
$$</p>
<p>Lastly, while the exact results from the theoretical analysis above should be plugged into $\eqref{eq:Obj_VDM}$ directly to complete the objective, in practice we often turn to the reweighted version of them (related to the idea of reweighted <abbr title="Evidence Lower BOund">ELBO</abbr>) for performace improvement. For example, in the case where $e(i, z_i)$ is adopted, rather than</p>
<p>$$
\begin{aligned}
& Obj_{\text{VDM}}(z_0) \\
& = \underbrace{E_{\epsilon_1 \sim N(0, I)} [ - log (\sqrt{det(2\pi\sigma_1^2 I)}) ]}_{\text{a constant}} \\
& \phantom{=} - E_{\epsilon_1 \sim N(0, I)} [ \frac{1}{2\sigma_1^2} \frac{1-\bar{\alpha}_1}{\bar{\alpha}_1} \| e(1, z_1) - \epsilon_1 \|_2^2 ] \\
& \phantom{=} - \underbrace{KL [ q(z_T|z_0) \| p(z_T) ]}_{= 0 \text{ by design}} \\
& \phantom{=} - \sum_{i=2}^T E_{\epsilon_i \sim N(0, I)} [ \frac{\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)^2}{2 \sigma_i^2} \| e(i, z_i) - \epsilon_i \|_2^2 ]
\end{aligned}
$$</p>
<p>the heuristic below is more commonly used for <abbr title="Variational Diffusion Model">VDM</abbr> training:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Obj_{\text{heuristic VDM}}(z_0) \\
& = - E_{\epsilon_1 \sim N(0, I)} [ \| e(1, z_1) - \epsilon_1 \|_2^2 ] \\
& \phantom{=} - \sum_{i=2}^T E_{\epsilon_i \sim N(0, I)} [ \| e(i, z_i) - \epsilon_i \|_2^2 ] \\
& = - \sum_{i=1}^T E_{\epsilon_i \sim N(0, I)} [ \| e(i, z_i) - \epsilon_i \|_2^2 ]
\end{aligned}
\label{eq:Obj_heuristic_VDM}
\end{equation}
$$</p>
<h2 class="reference-title">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/2208.11970">Calvin Luo. Understanding Diffusion Models: A Unified Perspective. 2022.</a></li>
<li><a href="https://arxiv.org/abs/2010.02502">Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. 2020.</a></li>
</ul>

</br></br></br></br>

  </article>
</main>

</body>
</html>
